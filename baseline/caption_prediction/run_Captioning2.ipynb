{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2312213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/snranepuradewage/.lico_env/jupyter/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor,AutoTokenizer, ViTImageProcessor, AutoImageProcessor\n",
    "from PIL import Image\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4746a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"]=\"vit-gpt\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b96043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/users/snranepuradewage/roco-dataset-master/data-master/train_captions.csv')\n",
    "df_val = pd.read_csv('/users/snranepuradewage/roco-dataset-master/data-master/valid_captions.csv')\n",
    "df_test = pd.read_csv('/users/snranepuradewage/roco-dataset-master/data-master/test_captions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98ac70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"ID\"] = \"/users/snranepuradewage/roco-dataset-master/data-master/train/\" + df_train[\"ID\"]+ \".jpg\"\n",
    "df_val[\"ID\"] = \"/users/snranepuradewage/roco-dataset-master/data-master/valid/\" + df_val[\"ID\"]+ \".jpg\"\n",
    "df_test[\"ID\"] = \"/users/snranepuradewage/roco-dataset-master/data-master/test/\" + df_test[\"ID\"]+ \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2e26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, df_val], ignore_index=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "dataset_clef = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46539b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.71s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(28896, 2560)\n",
       "      (wpe): Embedding(1024, 2560)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-31): 32 x GPT2Block(\n",
       "          (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=7680, nx=2560)\n",
       "            (c_proj): Conv1D(nf=2560, nx=2560)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (crossattention): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=5120, nx=2560)\n",
       "            (q_attn): Conv1D(nf=2560, nx=2560)\n",
       "            (c_proj): Conv1D(nf=2560, nx=2560)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=10240, nx=2560)\n",
       "            (c_proj): Conv1D(nf=2560, nx=10240)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=28896, bias=False)\n",
       "  )\n",
       "  (enc_to_dec_proj): Linear(in_features=768, out_features=2560, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"vit_biomedlm_caption_model\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d465206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing step\n",
    "def tokenization_fn(captions, max_target_length):\n",
    "    \"\"\"Run tokenization on captions.\"\"\"\n",
    "    labels = tokenizer(captions, \n",
    "                        padding=\"max_length\", \n",
    "                        max_length=max_target_length, truncation=True).input_ids\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b486e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image preprocessing step\n",
    "def feature_extraction_fn(image_paths, check_image=True):\n",
    "    \"\"\"\n",
    "    Run feature extraction on images\n",
    "    If `check_image` is `True`, the examples that fails during `Image.open()` will be caught and discarded.\n",
    "    Otherwise, an exception will be thrown.\n",
    "    \"\"\"\n",
    "    model_inputs = {}\n",
    "\n",
    "    if check_image:\n",
    "        images = []\n",
    "        for image_path in image_paths:\n",
    "            i_image = Image.open(image_path)\n",
    "            if i_image.mode != \"RGB\":\n",
    "                i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "            images.append(i_image)\n",
    "\n",
    "    encoder_inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "    return encoder_inputs.pixel_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e711b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(examples, max_target_length, check_image = True):\n",
    "    \"\"\"Run tokenization + image feature extraction\"\"\"\n",
    "    image_paths = examples['ID']\n",
    "    captions = examples['Caption']    \n",
    "        \n",
    "    model_inputs = {}\n",
    "    # This contains image path column\n",
    "    model_inputs['labels'] = tokenization_fn(captions, max_target_length)\n",
    "    model_inputs['pixel_values'] = feature_extraction_fn(image_paths, check_image=check_image)\n",
    "\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed9e987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69862/69862 [07:42<00:00, 151.00 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9927/9927 [01:09<00:00, 142.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = dataset_clef.map(\n",
    "        function=preprocess_fn,\n",
    "        batched=True,\n",
    "        fn_kwargs={\"max_target_length\": 128},\n",
    "        num_proc=1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d4e7d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/snranepuradewage/.lico_env/jupyter/env/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "        predict_with_generate=False,\n",
    "        num_train_epochs=2,\n",
    "        #eval_steps=1000,\n",
    "        evaluation_strategy= \"no\",\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        output_dir=\"./image-captioning-output-vit-biomedlm-roco2\",\n",
    "        optim=\"adafactor\",\n",
    "        fp16=True,\n",
    "        report_to=\"wandb\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14cddc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d384f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "        nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "ignore_pad_token_for_loss = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f7a9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d1e44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds,  references=decoded_labels, use_stemmer=False)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [ np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "724b63c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2794924/1645767887.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['test'],\n",
    "    data_collator=default_data_collator\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d05934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train(resume_from_checkpoint=\"./image-captioning-output-vit-biomedlm-roco2/checkpoint-17000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b73533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"./image-captioning-output-vit-biomedlm-roco2\")\n",
    "# tokenizer.save_pretrained(\"./image-captioning-output-vit-biomedlm-roco2\")\n",
    "# feature_extractor.save_pretrained(\"./image-captioning-output-vit-biomedlm-roco2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a1f8c",
   "metadata": {},
   "source": [
    "Caption generation + evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5cc9082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"./image-captioning-output-vit-biomedlm-roco2\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(path).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "image_processor = AutoImageProcessor.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cff6af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = getattr(tokenizer, \"bos_token_id\", None) or tokenizer.eos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62eb0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.pad_token_id = model.config.pad_token_id\n",
    "model.generation_config.eos_token_id = model.config.eos_token_id\n",
    "model.generation_config.max_new_tokens = 64\n",
    "model.generation_config.num_beams = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59e002d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_dir = \"/users/snranepuradewage/roco-dataset-master/data-master/test/\"\n",
    "df_test = pd.read_csv('/users/snranepuradewage/roco-dataset-master/data-master/test_captions.csv')\n",
    "\n",
    "df_test[\"image_path\"] = df_test[\"ID\"].apply(lambda x: os.path.join(test_image_dir, f\"{x}.jpg\"))\n",
    "\n",
    "images = [Image.open(p).convert(\"RGB\") for p in df_test[\"image_path\"]]\n",
    "captions_true = df_test[\"Caption\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "710b61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate captions ---\n",
    "preds = []\n",
    "for i, image in enumerate(images[:100]):   # try first 100 for speed\n",
    "    inputs = image_processor(image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs.pixel_values.to(\"cuda\")\n",
    "    attention_mask = torch.ones(pixel_values.shape[:-1], dtype=torch.long).to(\"cuda\")  # dummy mask\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        pixel_values=pixel_values,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        max_new_tokens=64,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    preds.append(caption.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd7db3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bertscore = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd28c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "result_rouge = rouge.compute(predictions=preds, references=captions_true[:len(preds)])\n",
    "result_bleu = bleu.compute(predictions=preds, references=captions_true[:len(preds)])\n",
    "result_bertscore = bertscore.compute(predictions=preds, references=captions_true[:len(preds)], lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69a2cbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.23708412701572446, 'rouge2': 0.08168826226621287, 'rougeL': 0.20921788797644297, 'rougeLsum': 0.20900434415085767} \n",
      "\n",
      "{'bleu': 0.03418002835753525, 'precisions': [0.24855491329479767, 0.0719207579672696, 0.02025202520252025, 0.003770028275212064], 'brevity_penalty': 1.0, 'length_ratio': 1.0288870008496176, 'translation_length': 2422, 'reference_length': 2354} \n",
      "\n",
      "[0.8490434288978577, 0.8765621781349182, 0.8977518081665039, 0.8716443181037903, 0.8715817332267761, 0.8917078375816345, 0.889004111289978, 0.8263463377952576, 0.8919671773910522, 0.8868426084518433]\n"
     ]
    }
   ],
   "source": [
    "print(result_rouge,'\\n')\n",
    "print(result_bleu,'\\n')\n",
    "print(result_bertscore[\"f1\"][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf530aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
