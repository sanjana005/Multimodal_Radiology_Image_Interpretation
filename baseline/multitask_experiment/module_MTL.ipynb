{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from timm import create_model\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b842999",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/users/snranepuradewage/roco-dataset-master/data-master\"\n",
    "MODEL_DIR = \"/users/snranepuradewage/roco_multimodal/baseline/multitask_experiment/Models_Multitask\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f25714",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_train_path = DATA_DIR + \"/train_concepts.csv\"\n",
    "caption_train_path = DATA_DIR + \"/train_captions.csv\"\n",
    "\n",
    "concept_valid_path = DATA_DIR + \"/valid_concepts.csv\"\n",
    "caption_valid_path = DATA_DIR + \"/valid_captions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfcdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concept_train = pd.read_csv(concept_train_path, sep=\",\")\n",
    "df_caption_train = pd.read_csv(caption_train_path, sep=\",\")\n",
    "\n",
    "df_concept_valid = pd.read_csv(concept_valid_path, sep=\",\")\n",
    "df_caption_valid = pd.read_csv(caption_valid_path, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on image ID\n",
    "df_train_merged = pd.merge(df_concept_train, df_caption_train, on=\"ID\")\n",
    "df_valid_merged = pd.merge(df_concept_valid, df_caption_valid, on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6256a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_merged.to_csv(DATA_DIR + \"/train_multitask.csv\", index=False)\n",
    "df_valid_merged.to_csv(DATA_DIR + \"/valid_multitask.csv\", index=False)\n",
    "\n",
    "print(\"Created multitask CSVs successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_DIR + \"/train_multitask.csv\")\n",
    "df_valid = pd.read_csv(DATA_DIR + \"/valid_multitask.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbae986",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 300\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE * 1.25)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomCrop((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89919366",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e995b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "if hf_tokenizer.pad_token is None:\n",
    "    hf_tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "PAD_IDX = hf_tokenizer.pad_token_id\n",
    "VOCAB_SIZE = hf_tokenizer.vocab_size\n",
    "\n",
    "print(\"Tokenizer vocab size:\", VOCAB_SIZE, \"| PAD_IDX:\", PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf35ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, labels, captions = zip(*batch)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "    labels = torch.stack(labels, 0)\n",
    "\n",
    "    captions = pad_sequence(captions, batch_first=True, padding_value=PAD_IDX)\n",
    "    return imgs, labels, captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuis_list = []\n",
    "\n",
    "for (i, row) in pd.concat([df_train, df_valid]).iterrows():\n",
    "    for cui in str(row[\"CUIs\"]).split(\";\"):\n",
    "        cui = cui.strip()\n",
    "        if cui not in cuis_list and cui != \"\":\n",
    "            cuis_list.append(cui)\n",
    "\n",
    "NUM_CONCEPTS = len(cuis_list)\n",
    "print(f\"Total unique CUIs (concepts): {NUM_CONCEPTS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15debce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "class RocoMTLDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, tokenizer=None, split=\"train\", max_len=MAX_LEN):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.split = split\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_dir = \"train\" if self.split == \"train\" else \"valid\"\n",
    "        img_path = os.path.join(DATA_DIR, img_dir, row[\"ID\"] + \".jpg\")\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Concept labels encoding\n",
    "        label_enc = torch.zeros(NUM_CONCEPTS)\n",
    "        for cui in row[\"CUIs\"].split(\";\"):\n",
    "            if cui in cuis_list:\n",
    "                label_enc[cuis_list.index(cui)] = 1\n",
    "\n",
    "        text = str(row[\"Caption\"])\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        tokens = torch.tensor(enc[\"input_ids\"], dtype=torch.long)\n",
    "\n",
    "        return img, label_enc, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a3bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_concepts, vocab_size, embed_dim=512, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = create_model('efficientnet_b0', pretrained=True, num_classes=0)\n",
    "        \n",
    "        # Concept detection head\n",
    "        self.concept_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.encoder.num_features, num_concepts)\n",
    "        )\n",
    "\n",
    "        # Caption decoder\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, images, captions=None):\n",
    "        feats = self.encoder.forward_features(images)\n",
    "        concept_logits = self.concept_head(feats)\n",
    "\n",
    "        if captions is not None:\n",
    "            emb = self.embed(captions)\n",
    "            out, _ = self.lstm(emb)\n",
    "            caption_logits = self.linear(out)\n",
    "        else:\n",
    "            caption_logits = None\n",
    "\n",
    "        return concept_logits, caption_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ba418",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RocoMTLDataset(df_train, transform=train_transform, tokenizer=hf_tokenizer, split=\"train\", max_len=MAX_LEN)\n",
    "valid_dataset = RocoMTLDataset(df_valid, transform=valid_transform, tokenizer=hf_tokenizer, split=\"valid\", max_len=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd37ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskModel(num_concepts=NUM_CONCEPTS, vocab_size=VOCAB_SIZE).to(device)\n",
    "\n",
    "criterion_concept = nn.BCEWithLogitsLoss()\n",
    "criterion_caption = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "alpha, beta = 0.7, 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93919cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_total, running_concept, running_caption = 0.0, 0.0, 0.0 \n",
    "\n",
    "    for images, concept_labels, captions in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        images, concept_labels, captions = images.to(device), concept_labels.to(device), captions.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        concept_logits, caption_logits = model(images, captions[:, :-1])\n",
    "\n",
    "        loss_concept = criterion_concept(concept_logits, concept_labels)\n",
    "        loss_caption = criterion_caption(\n",
    "            caption_logits.reshape(-1, caption_logits.size(-1)),\n",
    "            captions[:, 1:].reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss_total = alpha * loss_concept + beta * loss_caption\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_total += loss_total.item()\n",
    "        running_concept += loss_concept.item()\n",
    "        running_caption += loss_caption.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | total: {running_total/len(train_loader):.4f} | \"\n",
    "          f\"concept: {running_concept/len(train_loader):.4f} | \"\n",
    "          f\"caption: {running_caption/len(train_loader):.4f} | \"\n",
    "          f\"lr: {scheduler.get_last_lr()[0]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cabe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "all_labels, all_preds = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, concept_labels, captions in tqdm(valid_loader, desc=\"Evaluating concept head\"):\n",
    "        images = images.to(device)\n",
    "        concept_labels = concept_labels.to(device)\n",
    "        concept_logits, _ = model(images, captions[:, :-1].to(device))\n",
    "        preds = (torch.sigmoid(concept_logits) > 0.5).long()\n",
    "\n",
    "        all_labels.append(concept_labels.cpu())\n",
    "        all_preds.append(preds.cpu())\n",
    "\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "\n",
    "f1_samples = f1_score(all_labels, all_preds, average='samples')\n",
    "\n",
    "print(f\"Concept Detection F1 (samples): {f1_samples:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879405a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image_tensor, tokenizer, max_len=30):\n",
    "    model.eval()\n",
    "\n",
    "    if tokenizer.cls_token_id is not None:\n",
    "        generated_ids = [tokenizer.cls_token_id]\n",
    "    elif tokenizer.bos_token_id is not None:\n",
    "        generated_ids = [tokenizer.bos_token_id]\n",
    "    else:\n",
    "        generated_ids = [tokenizer.convert_tokens_to_ids(\"[CLS]\")]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            input_tensor = torch.tensor(generated_ids, dtype=torch.long).unsqueeze(0).to(image_tensor.device)\n",
    "            _, logits = model(image_tensor.unsqueeze(0), input_tensor)\n",
    "\n",
    "            next_token = logits[0, -1].argmax(-1).item()\n",
    "\n",
    "            if next_token in [tokenizer.sep_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id]:\n",
    "                break\n",
    "            generated_ids.append(next_token)\n",
    "\n",
    "    caption_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return caption_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "bleu1_scores, bleu4_scores, rougeL_scores = [], [], []\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "rouge_eval = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "for images, _, captions in tqdm(valid_loader, desc=\"Evaluating Captions\"):\n",
    "    images = images.to(device)\n",
    "    for i in range(images.size(0)):\n",
    "        ref_caption = hf_tokenizer.decode(captions[i].cpu().numpy(), skip_special_tokens=True)\n",
    "        pred_caption = generate_caption(model, images[i], hf_tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "        bleu1 = sentence_bleu([ref_caption.split()], pred_caption.split(),\n",
    "                              weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
    "        bleu4 = sentence_bleu([ref_caption.split()], pred_caption.split(),\n",
    "                              weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
    "        rougeL = rouge_eval.score(ref_caption, pred_caption)['rougeL'].fmeasure\n",
    "\n",
    "        bleu1_scores.append(bleu1)\n",
    "        bleu4_scores.append(bleu4)\n",
    "        rougeL_scores.append(rougeL)\n",
    "\n",
    "print(f\"BLEU-1:   {np.mean(bleu1_scores):.4f}\")\n",
    "print(f\"BLEU-4:   {np.mean(bleu4_scores):.4f}\")\n",
    "print(f\"ROUGE-L:  {np.mean(rougeL_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc44361",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(valid_loader))\n",
    "imgs, _, _ = sample\n",
    "img0 = imgs[0].to(device)\n",
    "\n",
    "pred_caption = generate_caption(model, img0, hf_tokenizer, max_len=MAX_LEN)\n",
    "print(\"Generated Caption:\", pred_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f0d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(imgs[0].permute(1, 2, 0))  \n",
    "plt.axis('off')\n",
    "plt.title(pred_caption)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), MODEL_DIR + \"multitask_model_final.pth\")\n",
    "\n",
    "# import pickle\n",
    "# with open(MODEL_DIR + \"tokenizer.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tokenizer, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
